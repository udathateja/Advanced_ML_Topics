{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wolpert in 1992 introduced Stacking. It involves:\n",
    "1. Splitting the training data set into two disjoint sets.\n",
    "2. Train several base learners on the first part.\n",
    "3. Make predictions with the base learners on the second (validation)part.\n",
    "4. Using predictions from the 3 rd step as the input to train a high learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/teja/Machine_Learning/master/Images/Stacking1.png\" width=\"640\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/teja/Machine_Learning/master/Images/Stacking2.png\" width=\"740\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score,log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset- https://www.kaggle.com/c/otto-group-product-classification-challenge\n",
    "The Otto Group is one of the worldâ€™s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line.\n",
    "\n",
    "A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range.\n",
    "\n",
    "dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data fields\n",
    "\n",
    "    id - an anonymous id unique to a product\n",
    "    feat_1, feat_2, ..., feat_93 - the various features of a product\n",
    "    target - the class of a product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/teja/Data_Files/master/otto_group/train.csv\")\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/teja/Data_Files/master/otto_group/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61878, 95)\n",
      "(144368, 94)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
       "       'Class_7', 'Class_8', 'Class_9'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify the classes\n",
    "df_train.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class_2    16122\n",
       "Class_6    14135\n",
       "Class_8     8464\n",
       "Class_3     8004\n",
       "Class_9     4955\n",
       "Class_7     2839\n",
       "Class_5     2739\n",
       "Class_4     2691\n",
       "Class_1     1929\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the numbers of samples across each classes\n",
    "df_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class_1</th>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_2</th>\n",
       "      <td>16122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_3</th>\n",
       "      <td>8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_4</th>\n",
       "      <td>2691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_5</th>\n",
       "      <td>2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_6</th>\n",
       "      <td>14135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_7</th>\n",
       "      <td>2839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_8</th>\n",
       "      <td>8464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_9</th>\n",
       "      <td>4955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        1\n",
       "target        \n",
       "Class_1   1929\n",
       "Class_2  16122\n",
       "Class_3   8004\n",
       "Class_4   2691\n",
       "Class_5   2739\n",
       "Class_6  14135\n",
       "Class_7   2839\n",
       "Class_8   8464\n",
       "Class_9   4955"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the numbers of samples across each classes\n",
    "pd.crosstab(df_train.target,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets replace Class_1 to 1 ,, do it for all 9 classes\n",
    "df_train[\"target\"] = df_train[\"target\"].str.replace(\"Class_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.head(2)   ## Data type is object , need to be int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets change the dtype - object to int\n",
    "df_train[\"target\"] = pd.to_numeric(df_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets define the X and y\n",
    "X  = df_train.iloc[:,1:-1]\n",
    "y = df_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61878, 93)\n",
      "(61878,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the Trainning Data in train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93)\n",
      "(49502,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "(12376, 93)\n",
      "(12376,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function to Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(y_true,y_pred1,y_pred2):\n",
    "    f1score = f1_score(y_true,y_pred1,average=\"weighted\")\n",
    "    print(\"F1 Score : \",f1score)\n",
    "    logloss = log_loss(y_true,y_pred2,eps=1e-15, normalize=True)\n",
    "    print(\"Log Loss for Predicted Probabilities : \",logloss)\n",
    "## This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks,\n",
    "## defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training \n",
    "## data y_true. The log loss is only defined for two or more labels. For a single sample with true \n",
    "## label yt in {0,1} and estimated probability yp that yt = 1, the log loss is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets train base learneres on full X_train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.33 s\n",
      "F1 Score :  0.7276665392288761\n",
      "Log Loss for Predicted Probabilities :  0.6988421299318078\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(max_iter=40000,tol=0.001,loss=\"log\")\n",
    "%time sgd_clf.fit(X_train,y_train)\n",
    "## lets predicts the y_pred1 and y_pred2\n",
    "sgd_y_pred1 = sgd_clf.predict(X_test)\n",
    "sgd_y_pred2 = sgd_clf.predict_proba(X_test)\n",
    "### Lets Evalaute Model\n",
    "model_evaluate(y_test,sgd_y_pred1,sgd_y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.76 s\n",
      "F1 Score :  0.7739562191425366\n",
      "Log Loss for Predicted Probabilities :  2.3086776076579794\n"
     ]
    }
   ],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "%time knn_clf.fit(X_train,y_train)\n",
    "knn_pred1 = knn_clf.predict(X_test)\n",
    "knn_pred2 = knn_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,knn_pred1,knn_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n",
      "F1 Score :  0.800866394700877\n",
      "Log Loss for Predicted Probabilities :  0.6014111699538554\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "%time rf_clf.fit(X_train,y_train)\n",
    "rf_pred1 = rf_clf.predict(X_test)\n",
    "rf_pred2 = rf_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,rf_pred1,rf_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ExtraTree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.88 s\n",
      "F1 Score :  0.7941928401694024\n",
      "Log Loss for Predicted Probabilities :  0.6891734010963404\n"
     ]
    }
   ],
   "source": [
    "Extree_clf = ExtraTreesClassifier(n_estimators=50)\n",
    "%time Extree_clf.fit(X_train,y_train)\n",
    "Extree_pred1 = Extree_clf.predict(X_test)\n",
    "Extree_pred2 = Extree_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,Extree_pred1,Extree_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilayer Perceptron Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 53s\n",
      "F1 Score :  0.7739095996725924\n",
      "Log Loss for Predicted Probabilities :  0.8608981965927792\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(max_iter=400)\n",
    "%time mlp_clf.fit(X_train,y_train)\n",
    "mlp_pred1 = mlp_clf.predict(X_test)\n",
    "mlp_pred2 = mlp_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,mlp_pred1,mlp_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.55 s\n",
      "F1 Score :  0.6744072921883629\n",
      "Log Loss for Predicted Probabilities :  2.0261936531308335\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier()\n",
    "%time ada_clf.fit(X_train,y_train)\n",
    "ada_pred1 = ada_clf.predict(X_test)\n",
    "ada_pred2 = ada_clf.predict_proba(X_test)\n",
    "## It's important to remember log loss does not have an upper bound. Log loss exists on the range [0, âˆž)\n",
    "model_evaluate(y_test,ada_pred1,ada_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaiveBayes Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 116 ms\n",
      "F1 Score :  0.6286302230265768\n",
      "Log Loss for Predicted Probabilities :  7.251880092416661\n"
     ]
    }
   ],
   "source": [
    "nb_clf = GaussianNB()\n",
    "%time nb_clf.fit(X_train,y_train)\n",
    "nb_pred1 = nb_clf.predict(X_test)\n",
    "nb_pred2 = nb_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,nb_pred1,nb_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.7578247463382258\n",
      "Log Loss for Predicted Probabilities :  0.6483268005374851\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(num_class = 9,objective=\"multi:softprob\",eval_metric=\"mlogloss\")\n",
    "%time xgb_clf.fit(X_train,y_train)\n",
    "xgb_pred1 = xgb_clf.predict(X_test)\n",
    "xgb_pred2 = xgb_clf.predict_proba(X_test)\n",
    "model_evaluate(y_test,xgb_pred1,xgb_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically both softmax and softprob are used for multiclass classification. Itâ€™s the output which separates them. In Softmax you will get the class with the maximum probability as output, but with Softprob you will get a matrix with probability value of each class you are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Classifier with GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, eval_metric='mlogloss',\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=3,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=100, n_jobs=8, nthread=None,\n",
       "                                     num_class=9, objective='multi:softprob',\n",
       "                                     random_state=0...bda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'colsample_bytree': [0.8], 'gamma': [0.03],\n",
       "                         'learning_rate': [0.08], 'max_depth': [10],\n",
       "                         'min_child_weight': [5], 'n_estimators': [300],\n",
       "                         'subsample': [0.85]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs_clf = XGBClassifier(num_class = 9,objective=\"multi:softprob\",eval_metric=\"mlogloss\",n_jobs=8)\n",
    "gs_param = {'max_depth': [10], 'n_estimators': [300], 'gamma': [0.03], 'learning_rate': [0.08], 'min_child_weight': [5], 'colsample_bytree': [0.8], 'subsample': [0.85]}\n",
    "gs = GridSearchCV(xgb_gs_clf,param_grid=gs_param,cv =2, scoring=\"neg_log_loss\")\n",
    "gs.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.8246957240832267\n",
      "Log Loss for Predicted Probabilities :  0.44924238754435664\n"
     ]
    }
   ],
   "source": [
    "xgb_gs_pred1 = gs.predict(X_test)\n",
    "xgb_gs_pred2 = gs.predict_proba(X_test)\n",
    "model_evaluate(y_test,xgb_gs_pred1,xgb_gs_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create New Dataset using predicted value from base learners**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SGD</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Random Foreset</th>\n",
       "      <th>Extree</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Adaboost</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>XGBoost_GS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SGD  KNN  Random Foreset  Extree  MLP  Adaboost  NaiveBayes  XGBoost_GS\n",
       "0    2    2               2       2    7         2           4           7\n",
       "1    7    7               7       7    7         1           3           7\n",
       "2    6    6               6       6    6         6           6           6\n",
       "3    4    6               6       6    6         4           9           6\n",
       "4    6    6               6       6    6         6           6           6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = pd.DataFrame({\"SGD\":sgd_y_pred1,\"KNN\":knn_pred1,\n",
    "                           \"Random Foreset\":rf_pred1,\"Extree\":Extree_pred1,\n",
    "                           \"MLP\":mlp_pred1,\"Adaboost\":ada_pred1,\n",
    "                           \"NaiveBayes\":nb_pred1,\"XGBoost_GS\":xgb_gs_pred1})\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS I have mentioned in starting graph that stacking uses predictions of base learning classifiers as input for training  to a second-level model. However we cannot simply train the base models on the full training data, generate predictions on the full test data set and then output these for the second-level model training. As we have seen that there are to much difference in F1 score and log_loss among base learning models.\n",
    "We will use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Buils Stacking Model using splitting the trainng data for Base Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets convert the X_train,X_test,y_train,y_test data type in numpy ndarray\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = y_train.values.ravel()\n",
    "y_test_np = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93)\n",
      "(49502,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(12376, 93)\n",
      "(12376,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_np.shape)\n",
    "print(y_train_np.shape)\n",
    "print(type(X_train_np))\n",
    "print(type(y_train_np))\n",
    "print(X_test_np.shape)\n",
    "print(y_test_np.shape)\n",
    "print(type(X_test_np))\n",
    "print(type(y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5  ## As we are using 5 base learner\n",
    "n_class = len(df_train.target.unique())\n",
    "kf = KFold(n_splits= n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get Train and Test data using K-Fold\n",
    "def get_train_test(clf, x_train, y_train, x_test):   ## Value of x_train etc should be in np.ndarray\n",
    "    train_stage1 = np.zeros((x_train.shape[0],n_class)) ## To get traiing data (row count,9)\n",
    "    ## to check th zero count in np.array   np.count_nonzero(train_stage1==0)\n",
    "    test_stage1 = np.zeros((x_test.shape[0],n_class))  ## To get testing data (row count,9)\n",
    "   #### oof_test_temp = np.empty((n_folds, ntest))\n",
    "   \n",
    "    for i,(train_index,test_index) in enumerate(kf.split(x_train)):\n",
    "        \n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        y_te = y_train[test_index]\n",
    "        \n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        pred_test1 = clf.predict_proba(x_te)\n",
    "        train_stage1[test_index,:] = pred_test1  ### for 1st stage training data\n",
    "        \n",
    "        pred_test2 = clf.predict_proba(x_test)  \n",
    "        test_stage1 =  test_stage1 + pred_test2  ### for 1st stage test data\n",
    "\n",
    "    return train_stage1, test_stage1/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets take the base learning model\n",
    "### Stage 1 Model\n",
    "base1_clf = sgd_clf\n",
    "base2_clf = mlp_clf\n",
    "base3_clf = knn_clf\n",
    "base4_clf = Extree_clf\n",
    "base5_clf = ada_clf\n",
    "### Stage 2 Model\n",
    "final_clf = rf_clf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1_train,base1_test = get_train_test(base1_clf,X_train_np,y_train_np,X_test_np)\n",
    "base2_train,base2_test = get_train_test(base2_clf,X_train_np,y_train_np,X_test_np)\n",
    "base3_train,base3_test = get_train_test(base3_clf,X_train_np,y_train_np,X_test_np)\n",
    "base4_train,base4_test = get_train_test(base4_clf,X_train_np,y_train_np,X_test_np)\n",
    "base5_train,base5_test = get_train_test(base5_clf,X_train_np,y_train_np,X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now use output of these base learners as input for final model\n",
    "X_train_f = np.concatenate((base1_train,base2_train,base3_train,base4_train,base5_train),axis = 1)\n",
    "X_test_f = np.concatenate((base1_test,base2_test,base3_test,base4_test,base5_test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49502, 45), (12376, 45))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_f.shape,X_test_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets train the final model\n",
    "%time final_clf.fit(X_train_f,y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.8290447215920956\n",
      "Log Loss for Predicted Probabilities :  0.5566684256504653\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the final model\n",
    "final_pred1 = final_clf.predict(X_test_f)\n",
    "final_pred2 = final_clf.predict_proba(X_test_f)\n",
    "model_evaluate(y_test,final_pred1,final_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observer after stacking we get better F1 score and loss is decreased.If we will use XGB probably we will get better F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
